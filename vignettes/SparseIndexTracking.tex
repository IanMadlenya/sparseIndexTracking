\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Design of Portfolio of Stocks to Track an Index},
            pdfauthor={Konstantinos Benidis and Daniel P. Palomar},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Design of Portfolio of Stocks to Track an Index}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Konstantinos Benidis and Daniel P. Palomar}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-04-23}

\setlength{\parindent}{12pt} \usepackage{graphicx} \usepackage{bm}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

This vignette illustrates the design of sparse portfolios that aim to
track a financila index with the package \texttt{sparseIndexTracking}
(with a comparison with other packages) and gives a description of the
algorithms used.

\section{Comparison with other
packages}\label{comparison-with-other-packages}

\section{Usage of the package}\label{usage-of-the-package}

\section{Explanation of the
algorithms}\label{explanation-of-the-algorithms}

\subsection{\texorpdfstring{\texttt{spIndexTrack()}: Sparse portfolio
construction}{spIndexTrack(): Sparse portfolio construction}}\label{spindextrack-sparse-portfolio-construction}

Assume that an index is composed of \(N\) assets. We denote by
\(\mathbf{r}^b=[r_1^b,\dots,r_T^b]^\top\in\mathbb{R}^T\) and
\(\mathbf{X}=[\mathbf{r}_1,\dots,\mathbf{r}_T]^\top\in\mathbb{R}^{T\times N}\)
the (arithmetic) net returns of the index and the \(N\) assets in the
past \(T\) days, respectively, with \(\mathbf{r}_t\in\mathbb{R}^N\)
denoting the net returns of the \(N\) assets at the \(t\)-th day.

The goal of \texttt{spIndexTrack()} is the design of a (sparse)
portfolio \(\mathbf{w}\in\mathbb{R}_+^N\), with
\(\mathbf{w}^\top\mathbf{1} = 1\), that tracks closely the index, i.e.,
\(\mathbf{X}\mathbf{w} \approx \mathbf{r}^b\), based on {[}1{]}. The
underlying optimization problem that is solved is \[\begin{equation}
\begin{aligned}\label{opt:joint_index_tracking}
    &\underset{\mathbf{w}}{\text{minimize}}\quad \text{TE}(\mathbf{w}) + \lambda\|\mathbf{w}\|_0\\
    &\text{subject to}\quad \mathbf{w}^\top\mathbf{1}=1,\\
    &\hspace{25mm} \mathbf{0}\leq\mathbf{w}\leq u\mathbf{1},
\end{aligned}
\end{equation}\] where \(\text{TE}(\mathbf{w})\) is a general tracking
error (we will see specific tracking errors shortly), \(\lambda\) is a
regularization parameter that controls the sparsity of the portfolio,
and \(u\) is an upper bound on the weights of the portfolio.

The \(\ell_0\)-``norm'' is approximated by the continuous and
differentiable function (for \(\mathbf{w} \geq \mathbf{0}\)) \[
\begin{equation}\label{eq:approx_func_u}
\rho_{p,u}(w) = \frac{\log(1 + w/p)}{\log(1 + u/p)},
\end{equation} 
\] where \(p>0\) is a parameter that controls the approximation. This
leads to the following approximate problem: \[\begin{equation}
\begin{aligned}\label{opt:joint_index_tracking_approx}
    &\underset{\mathbf{w}}{\text{minimize}}\quad \text{TE}(\mathbf{w}) + \lambda\mathbf{1}^\top\bm{\rho}_{p,u}(\mathbf{w})\\
    &\text{subject to}\quad \mathbf{w}^\top\mathbf{1}=1,\\
    &\hspace{25mm} \mathbf{0}\leq\mathbf{w}\leq u\mathbf{1},
\end{aligned}
\end{equation}\] where
\(\bm{\rho}_{p,u}(\mathbf{w})=[\mathbf{\rho}_{p,u}(w_1),\dots,\rho_{p,u}(w_N)]^\top\).

There are four available tracking errors \(\text{TE}(\mathbf{w}\)) in
\texttt{spIndexTrack()}:

\begin{itemize}
\tightlist
\item
  Empirical tracking error (ETE):
\end{itemize}

\[\begin{equation}\label{eq:ETE}
\text{ETE}(\mathbf{w}) = \frac{1}{T}\big\|\mathbf{r}^b - \mathbf{X}\mathbf{w}\big\|_2^2
\end{equation} \]

\begin{itemize}
\tightlist
\item
  Downside risk (DR):
\end{itemize}

\[\begin{equation}\label{eq:DR}
\text{DR}(\mathbf{w}) = \frac{1}{T}\big\|(\mathbf{r}^b-\mathbf{X}\mathbf{w})^+\big\|_2^2
\end{equation}\]

\begin{itemize}
\tightlist
\item
  Huber empirical tracking error (HETE):
\end{itemize}

\[\begin{equation}
\text{HETE}(\mathbf{w}) = \frac{1}{T}\mathbf{1}^\top\bm{\phi}\left(\mathbf{r}^b - \mathbf{X}\mathbf{w}\right),\label{eq:huber-ETE}
\end{equation}\]

\begin{itemize}
\tightlist
\item
  Huber downside risk (HDR):
\end{itemize}

\[\begin{equation}
\text{HDR}(\mathbf{w}) = \frac{1}{T}\mathbf{1}^\top\bm{\phi}\left((\mathbf{r}^b-\mathbf{X}\mathbf{w})^+\right),\label{eq:huber-DR}
\end{equation}\] where
\(\bm{\phi}(\mathbf{x}) = [\phi(x_1), \dots, \phi(x_T)]^\top\), and
\[\begin{equation}\label{eq:Huber}
\phi(x) = \begin{cases}
x^2, &\quad |x| \leq M,\\
M(2|x| - M), &\quad |x| > M,
\end{cases}
\end{equation}\] with \(M>0\) being the Huber parameter.

Regardless of the selected tracking error measure this problem can be
solved via Majorization-Minimization (MM) {[}2{]} with an iterative
closed-form update algorithm. It can be shown that all of the above
variations boil down to the optimization of the following convex
problem: \[\begin{equation}
\begin{aligned}\label{opt:general_form}
&\underset{\mathbf{w}}{\text{minimize}}\quad \mathbf{w}^\top\mathbf{w} + \mathbf{q}^\top\mathbf{w}\\
&\text{subject to}\quad \mathbf{w}\in\mathcal{W}_{u},
\end{aligned}
\end{equation}\] where \[\begin{equation}\label{eq:Wu}
\mathcal{W}_{u} = \big\{\mathbf{w} \big| \mathbf{w}^\top\mathbf{1} = 1, \mathbf{0}\leq\mathbf{w}\leq u\mathbf{1}\big\},
\end{equation}\] and \(\mathbf{q}\in\mathbb{R}^N\).

ADD TABLE WITH THE DIFFERENT \(\mathbf{q}\) FOR EACH TE.

HOW CAN WE ADD PROPOSITIONS?

The following propositions provide a waterfilling structured solution of
\eqref{opt:general_form}, considering two special cases, namely, \(u=1\)
and \(u<1\).

\[
\begin{proposition}\label{prop:closedform}
    The optimal solution of the optimization problem \eqref{opt:general_form} with $u=1$ is
    \begin{equation}
    \mathbf{w}^\star = \left(-\frac{1}{2}(\mu\mathbf{1} + \mathbf{q})\right)^+,
    \end{equation}
    with
    \begin{equation}
    \mu = -\frac{\sum_{i\in\mathcal{A}}q_i + 2}{\text{card}(\mathcal{A})},
    \end{equation}
    and
    \begin{equation}
    \mathcal{A} = \big\{j \big| \mu + q_j < 0\big\},
    \end{equation}
    where $\mathcal{A}$ can be determined in $O(\log(N))$ steps. 
\end{proposition}\] We refer to the iterative procedure of Proposition
\ref{prop:closedform} as AS\(_{1}(\mathbf{w})\) (Active-Set for
\(u=1\)).

\[
\begin{proposition}\label{prop:closedform_u}
    The optimal solution of the optimization problem \eqref{opt:general_form} with $u<1$ is
    \begin{equation}
    \mathbf{w}^\star = \left(\min\left(-\frac{1}{2}(\mu\mathbf{1} + \mathbf{q}),u\mathbf{1}\right)\right)^+,
    \end{equation}
    with 
    \begin{equation}\label{eq:mu_u}
    \mu = -\frac{\sum_{j\in\mathcal{B}_2}q_j + 2 - \text{card}(\mathcal{B}_1)2u}{\text{card}(\mathcal{B}_2)},
    \end{equation}
    and
    \begin{align}
    \mathcal{B}_1 =& \big\{j \big| \mu + q_j \leq -2u\big\},\\
    \mathcal{B}_2 =& \big\{j \big| -2u < \mu + q_j < 0\big\},
    \end{align}
    where $\mathcal{B}_1$ and $\mathcal{B}_2$ can be determined in $O(N\log(N))$ steps.
\end{proposition}\] We refer to the iterative procedure of Proposition
\ref{prop:closedform_u} as AS\(_u(\vq)\) (Active-Set for general
\(u<1\)).

The iterative closed-form update algorithm is:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set \(k=0\) and choose an initial point \(\mathbf{w}^{(0)}\)\\
\item
  Compute \(\mathbf{q}\) according to the selected tracking error\\
\item
  Find the optimal solution \(\mathbf{w}^\star\) with
  AS\(_{1|u}(\mathbf{q})\) and set it equal to \(\mathbf{w}^{(k+1)}\)
\item
  \(k \gets k+1\)\\
\item
  Repeat steps 2-4 until convergence\\
\item
  Return \(\mathbf{w}^{(k)}\)
\end{enumerate}
\end{quote}

The initial point of the algorithm \(\mathbf{U}^{(0)}\) is set by
default to the \(q\) leading standard eigenvectors, unless the user
specifies otherwise. Internally, all the computations of
\(\mathbf{G}^{(k)}\) and \(\mathbf{H}^{(k)}\) are done through the
eigenvalue decomposition (EVD) of \(\mathbf{S}\). Since we can also
retrieve the eigenvectors and eigenvalues of \(\mathbf{S}\) through the
singular value decomposition (SVD) of the data matrix \(\mathbf{X}\),
with \(\mathbf{S} = \frac{1}{n-1}\mathbf{X}^\top\mathbf{X}\), it becomes
possible to use as an input to \texttt{spEigen()} either the covariance
matrix \(\mathbf{S}\) or directly the data matrix \(\mathbf{X}\).

Finally, note that the approximate problem is controlled by the
parameter \(p\), and in particular, as \(p\rightarrow0\) we get
\(\rho_{p,u}\rightarrow\ell_0\). However, by setting small values to
\(p\), it is likely that the algorithm will get stuck to a local
minimum. To solve this issue we start with large values for \(p\), i.e.,
a ``loose'' approximation, and solve the corresponding optimization
problem. Then, we sequentially decrease \(p\), i.e., we ``tighten'' the
approximation, and solve the problem again using the previous solution
as an initial point. In practice we are interested only in the last,
``tightest'' problem. For each problem that is solved (i.e., for fixed
\(p\)) we utilize an acceleration scheme that increases the convergence
speed of the MM algorithm. For details, please refer to {[}3{]}.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\setlength{\parindent}{-0.2in} \setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt} \noindent

\hypertarget{refs}{}
\hypertarget{ref-BenFengPal2018}{}
{[}1{]} K. Benidis, Y. Feng, and D. P. Palomar, ``Sparse portfolios for
high-dimensional financial index tracking,'' \emph{IEEE Transactions on
Signal Processing}, vol. 66, no. 1, pp. 155--170, Jan. 2018.

\hypertarget{ref-SunBabPal2018}{}
{[}2{]} Y. Sun, P. Babu, and D. P. Palomar, ``Majorization-minimization
algorithms in signal processing, communications, and machine learning,''
\emph{IEEE Transactions on Signal Processing}, vol. 65, no. 3, pp.
794--816, Feb. 2017.

\hypertarget{ref-Varadhan2008}{}
{[}3{]} R. Varadhan and C. Roland, ``Simple and globally convergent
methods for accelerating the convergence of any em algorithm,''
\emph{Scandinavian Journal of Statistics}, vol. 35, no. 2, pp. 335--353,
2008.


\end{document}
